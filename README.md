# LLM-Learn
# 大模型学习
![image](https://github.com/jinbo0906/LLM-Learn/assets/85282296/aea764e0-8818-43bc-a7f1-fa984d92e1ee)
OpenAI一路走下来的**关键技术**：
1. GPT-1是第一次使用预训练方法来实现高效语言理解的训练；
2. GPT-2主要采用了迁移学习技术，能在多种任务中高效应用预训练信息，并进一步提高语言理解能力；
3. DALL·E是走到另外一个模态；
4. GPT-3主要注重泛化能力，few-shot（小样本）的泛化；
5. GPT-3.5 instruction following（指令遵循）和tuning（微调）是最大突破；
6. GPT-4 已经开始实现工程化。
7. 2023年3月的Plugin是生态化。

## Task 01
梳理了语言模型的进化史。
## Task 02
这一节主要是讲述大模型的能力，通过一些任务来探索，这里我整理了prompt的范式，包括一个范例，另外整理了langchain介绍，包括一些langchain示例。
## Task 03
这一部分主要整理了ChatGPT的原理，包括ChatGPT训练的三个阶段，目前这部分仅是简单叙述，接下来会对具体的技术细节进行总结。
## Task 04
这一部分主要梳理了GPT的进化史，从GPT-1到GPT-3，从数据、模型、训练方式有了一个大致的了解。
## Task 05
这一部分整理了ChatGPT从零开始的训练流程，主要包括预训练、指令微调、奖励模型三个阶段。
## Task 06
这一部分整理了prompt-Tuning的研究进展。
## Task 07
这一部分整理了思维链的相关知识，主要包括目前对于CoT的一些研究进展和局限。
